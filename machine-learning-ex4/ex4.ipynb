{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitbaseconda44314d158c3145d9a31ade35c93e9832",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex4: Neural Network Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CS229:C:\\data\\网络资源-吴恩达机器学习课程\\编程作业\\machine-learning-ex4\\ex4\n"
    }
   ],
   "source": [
    "from pathlib import *\n",
    "%run ../cs229config.py\n",
    "exppath = str(PurePath(cs229basepath(),'machine-learning-ex4','ex4'))\n",
    "print(\"CS229:\"+exppath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup the parameters you will use for this exercise\n",
    "input_layer_size  = 400  # 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25   # 25 hidden units\n",
    "num_labels = 10          # 10 labels, from 1 to 10   \n",
    "                         # (note that we have mapped \"0\" to label 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =========== Part 1: Loading and Visualizing Data =============\n",
    "  We start the exercise by first loading and visualizing the dataset. \n",
    "  You will be working with a dataset that contains handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Loading and Visualizing Data ...\n"
    }
   ],
   "source": [
    "# Load Training Data\n",
    "print('Loading and Visualizing Data ...')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "data = loadmat(PurePath(exppath,'ex4data1.mat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['X']\n",
    "y = data['y']\n",
    "data.keys()\n",
    "m = len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "5000"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "% Randomly select 100 data points to display\n",
    "sel = randperm(size(X, 1));\n",
    "sel = sel(1:100);\n",
    "\n",
    "displayData(X(sel, :));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ================ Part 2: Loading Parameters ================\n",
    " In this part of the exercise, we load some pre-initialized \n",
    " neural network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Loading Saved Neural Network Parameters ...\ndict_keys(['__header__', '__version__', '__globals__', 'Theta1', 'Theta2'])\n"
    }
   ],
   "source": [
    "print('Loading Saved Neural Network Parameters ...')\n",
    "# Load the weights into variables Theta1 and Theta2\n",
    "ex4weights = loadmat(PurePath(exppath,'ex4weights.mat'))\n",
    "print(ex4weights.keys())\n",
    "# Load the weights into variables Theta1 and Theta2\n",
    "Theta1 = ex4weights['Theta1']\n",
    "Theta2 = ex4weights['Theta2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Unroll parameters \n",
    "nn_params = [Theta1(:) ; Theta2(:)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ================ Part 3: Compute Cost (Feedforward) ================\n",
    "  To the neural network, you should first start by implementing the\n",
    "  feedforward part of the neural network that returns the cost only. You\n",
    "  should complete the code in nnCostFunction.m to return cost. After\n",
    "  implementing the feedforward to compute the cost, you can verify that\n",
    "  your implementation is correct by verifying that you get the same cost\n",
    "  as us for the fixed debugging parameters.\n",
    "\n",
    "  We suggest implementing the feedforward cost *without* regularization\n",
    "  first so that it will be easier for you to debug. Later, in part 4, you\n",
    "  will get to implement the regularized cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nFeedforward Using Neural Network ...\n\nProgram paused. Press enter to continue.\n"
    }
   ],
   "source": [
    "print('\\nFeedforward Using Neural Network ...\\n')\n",
    "# Weight regularization parameter (we set this to 0 here).\n",
    "Lambda = 0\n",
    "def nnCostFunction():\n",
    "    return\n",
    "# J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, ...\n",
    "#                    num_labels, X, y, lambda);\n",
    "\n",
    "# print(['Cost at parameters (loaded from ex4weights): %f '...\n",
    "#          '\\n(this value should be about 0.287629)\\n'], J)\n",
    "print('Program paused. Press enter to continue.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =============== Part 4: Implement Regularization ===============\n",
    "  Once your cost function implementation is correct, you should now\n",
    "  continue to implement the regularization with the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Checking Cost Function (w/ Regularization) ... \n"
    }
   ],
   "source": [
    "print('Checking Cost Function (w/ Regularization) ... ')\n",
    "# Weight regularization parameter (we set this to 1 here).\n",
    "Lambda = 1\n",
    "\n",
    "# J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, ...\n",
    "#                    num_labels, X, y, lambda);\n",
    "\n",
    "# fprintf(['Cost at parameters (loaded from ex4weights): %f '...\n",
    "#          '\\n(this value should be about 0.383770)\\n'], J);\n",
    "\n",
    "# fprintf('Program paused. Press enter to continue.\\n');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ================ Part 5: Sigmoid Gradient  ================\n",
    "  Before you start implementing the neural network, you will first\n",
    "  implement the gradient for the sigmoid function. You should complete the\n",
    "  code in the sigmoidGradient.m file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#SIGMOIDGRADIENT returns the gradient of the sigmoid function\n",
    "#evaluated at z\n",
    "#   g = SIGMOIDGRADIENT(z) computes the gradient of the sigmoid function\n",
    "#   evaluated at z. This should work regardless if z is a matrix or a\n",
    "#   vector. In particular, if z is a vector or matrix, you should return\n",
    "#   the gradient for each element.\n",
    "def sigmoidGradient(z):\n",
    "    return np.zeros(len(z))\n",
    "# % ====================== YOUR CODE HERE ======================\n",
    "# % Instructions: Compute the gradient of the sigmoid function evaluated at\n",
    "# %               each value of z (z can be a matrix, vector or scalar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Evaluating sigmoid gradient...\nSigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:  \nProgram paused. Press enter to continue.\n"
    }
   ],
   "source": [
    "print('Evaluating sigmoid gradient...')\n",
    "g = sigmoidGradient([-1,-0.5,0,0.5,1])\n",
    "print('Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:  ')\n",
    "# print('%f ', g)\n",
    "print('Program paused. Press enter to continue.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ================ Part 6: Initializing Pameters ================\n",
    "  In this part of the exercise, you will be starting to implment a two\n",
    "  layer neural network that classifies digits. You will start by\n",
    "  implementing a function to initialize the weights of the neural network\n",
    "  (randInitializeWeights.m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =============== Part 7: Implement Backpropagation ===============\n",
    "  Once your cost matches up with ours, you should proceed to implement the\n",
    "  backpropagation algorithm for the neural network. You should add to the\n",
    "  code you've written in nnCostFunction.m to return the partial\n",
    "  derivatives of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =============== Part 8: Implement Regularization ===============\n",
    "  Once your backpropagation implementation is correct, you should now\n",
    "  continue to implement the regularization with the cost and gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}